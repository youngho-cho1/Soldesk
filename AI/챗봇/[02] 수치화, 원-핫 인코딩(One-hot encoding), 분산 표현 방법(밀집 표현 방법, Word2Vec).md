1. 정수화
   - 일반적으로 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수를 내림차순으로 1부터 정수 부여
   - Python: 1, DBMS: 2, JAVA: 3: Python, JAVA: 객체지향 이론이 거의 비슷함으로 전혀 다른 언어는 아님, 단어들간의 '비슷한 정도 표현이 어려움'.


2. 원-핫 인코딩(One-hot encoding)
   - '원-핫 인코딩'으로 나온 값을 '원-핫 벡터'라고하며, 전체 요소중 단 하나의 값만 1임으로 '희소 벡터(희소 행렬)'하고함.
   - 각각의 분류에 속할 확률이 출력됨.
   - 단어가 100개이면 벡터의 크기는 100 차원이됨, 출력도 100가지가됨.
   - 다중 분류, 단어가 많아지면 메모리 낭비가 심하게 발생함.
![image](https://user-images.githubusercontent.com/74487628/178141374-f9949c2e-f40f-4f46-a6b2-bfbd6c3937ac.png)
2. 분산표현과 희소 표현
1) 희소 표현(One-hot encoding)
   - 분류 종류가 적은 경우 사용 권장
   - 분류 종류가 많다면 많은 차원을 사용해야하며 분류되는 값만 1을 갖으며 나머지는 0의 값을 갖는 비효율적인구조임.
   - 남자와 남성은 전혀 다른 단어로 분류됨, 의미 표현이 부족함.
   - 확률이 가장 높은 분류값을 찾아서 결정함, argmax() 사용.
![image](https://user-images.githubusercontent.com/74487628/178141385-e864385c-83f6-4658-9f5b-382c6411a5f4.png)
2) 분산 표현 방법(밀집 표현 방법, Word2Vec)
   - 적은 차원으로 대규모의 분류 표현 가능
![image](https://user-images.githubusercontent.com/74487628/178141391-7559cf8a-f41c-47ea-aae2-f169f9e7d475.png)
- 벡터(수치화) 공간에서 비슷한 의미를 갖는 단어들은 비슷한 위치에 분포됨으로 남자와 남성의 단어 위치는 매우 가까움.
     이런 두 단어간의 거리를 계산 할 수 있으면 컴퓨터는 '남자'와 '남성' 두 단어를 같은 의미로 해석 할 수 있어 유용한 기법임.
![image](https://user-images.githubusercontent.com/74487628/178141401-4a282431-738c-4dcd-bb27-fadcaf5cb4d5.png)
3. Word2Vec
   - 분포 가설(Distributed hypothesis)에 기반함
   - 분포 가설은 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다 라는 의미이다.
     따라서 어떤 글의 비슷한 위치에 존재하는 단어는 단어 간의 유사도를 높게 측정함.
   - 1에 가까울수록 유사도가 높음, 좋은 말뭉치(문장들의 모임) 모델을 사용시 언어 학습이 완성도있게 이루어짐.
   - 신경망 기반의 대표적인 '워드 임베딩' package ★
   - 2013년 Google에서 발표했으며 가장 많이 사용되는 워드 임베딩 방법
   - 기존의 신경망 모델과 비슷한 구조를 가지고 있으나 계산량을 획기적으로 줄여 빠른 학습을 가능하게함.
   - 사용법이 간단하며 성능이 나쁘지않은 Gensim 패키지를 많이 사용함.
   - Word2Vec을 만들기위해서는 한국어 말뭉치(단어의 집합)를 이용해야함.

1) CBOW 모델
   - 맥락이라 표현되는 주변 단어들을 이용해 타깃 단어를 예측하는 신경망 모델
   - 신경망의 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정해 학습된 가중치 데이터를 임베딩 벡터로 활용함.


2) skip-gram 모델
   - CBOW 모델과 반대로 하나의 타깃 단어를 이용해 주변 단어들을 예측하는 신경망 모델
   - CBOW 모델에 비해 예측해야 하는 맥락이 많아짐으로 단어 분산 표현력이 우수해 CBOW 모델에 비해 품질이 우수함.
   - CBOW 모델은 타깃 단어의 손실만 계산하면 됨으로 학습 속도가 빠른 장점이 있음.

3) 모델 비교
![image](https://user-images.githubusercontent.com/74487628/178141410-6cb9ebff-ebf0-4347-9063-d925ecc376db.png)
4) 윈도우 크기: 앞뒤로 몇개의 단어를 확인할 것인지의 크기
![image](https://user-images.githubusercontent.com/74487628/178141418-d6e7d83b-ad4f-44a1-8cd1-41767864630c.png)
5) Word2Vec은 의미상 비슷한 단어들을 비슷한 벡터 공간에 위치 시킴
![image](https://user-images.githubusercontent.com/74487628/178141427-c085d8c7-a914-4cbf-86a2-52799d4c6900.png)
4. Word2vec model 제작 실습
1) Word2Vec 하이퍼파라미터
   - sentences: Word2Vec 모델 학습에 필요한 문장 데이터
   - vector_size=200: 단어 임베딩 벡터의 차원(크기), 200개의 변수로 단어 표현
   - window: 주변 단어 윈도우의 크기
   - min_count: 단어 최소 빈도수 제한, 설정된 min_count 빈도수 이하의 단어들은 학습하지 않음.
   - sg: 0(CBOW 모델), 1(skip-gram 모델)



