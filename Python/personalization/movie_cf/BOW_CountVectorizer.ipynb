{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words(BoW)\n",
    "- Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
    "- BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이기 때문에, 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰임.\n",
    "- 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰임.\n",
    "- 가령, '달리기', '체력', '근력'과 같은 단어가 자주 등장하면 해당 문서를 체육 관련 문서로 분류할 수 있을 것임.\n",
    "- '미분', '방정식', '부등식'과 같은 단어가 자주 등장한다면 수학 관련 문서로 분류할 수 있음.\n",
    "- BoW를 만드는 과정<br>\n",
    "(1) 우선, 각 단어에 고유한 정수 인덱스를 부여합니다.<br>\n",
    "(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이번', '주', '월요일', '은', '휴강', '다음주', '월요일', '도', '휴강', '2', '틀', '간', '휴강']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt # Twitter 자연어 처리 package \n",
    "import re  # 정규 표현식 package \n",
    "okt=Okt()  \n",
    "\n",
    "# \".\"을 찾아서 \"\"로 변경, 정규 표현식을 통해 온점을 제거하는 정제 작업입니다. \n",
    "token=re.sub(\"(\\.)\",\"\",\"이번주 월요일은 휴강 다음주 월요일도 휴강 2틀간 휴강.\") \n",
    "# Okt 형태소(더이상 분리 할 수 없는 의미를 가지고 있는 단어) 분석기를 통해\n",
    "# 토큰화(분리) 작업을 수행\n",
    "token=okt.morphs(token)  \n",
    "\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={} # 키:값 \n",
    "bow=[]        # 배열 \n",
    "for voca in token: # '이번', '주', '월요일'...\n",
    "    if voca not in word2index.keys():   # 새로운 형태소가 key에 없다면\n",
    "        word2index[voca]=len(word2index) # '이번': 0, '주': 1, '월요일': 2... \n",
    "        # token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고,\n",
    "        # 이미 있는 단어는 넘깁니다.   \n",
    "        bow.insert(len(word2index)-1,1) # bow.insert(0, 1)\n",
    "        # 단어를 최초 등록중임으로 BoW 전체에 전부 기본값 1을 넣어줍니다. \n",
    "        # 단어의 개수 만큼 1이 list에 추가됨.\n",
    "    else: # word2index에 존재한다면\n",
    "        # 재등장하는 단어의 값을 가져와서 bow list의 인덱스로 사용\n",
    "        index=word2index.get(voca) \n",
    "        bow[index]=bow[index]+1 # 동일한 단어 등장시 카운트 1 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'이번': 0, '주': 1, '월요일': 2, '은': 3, '휴강': 4, '다음주': 5, '도': 6, '2': 7, '틀': 8, '간': 9}\n"
     ]
    }
   ],
   "source": [
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 1, 3, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수로 제작\n",
    "def bow_process(src):\n",
    "    okt=Okt()  \n",
    "    token=re.sub(\"[\\.%]\",\"\",src) # ., % 제거\n",
    "    token=okt.morphs(token) \n",
    "\n",
    "    print(token)\n",
    "\n",
    "    word2index={}  \n",
    "    bow=[]  \n",
    "    for voca in token: \n",
    "        if voca not in word2index.keys(): \n",
    "            word2index[voca]=len(word2index)\n",
    "            bow.insert(len(word2index)-1,1)\n",
    "        else:\n",
    "            index=word2index.get(voca)\n",
    "            bow[index]=bow[index]+1\n",
    "    return word2index, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['월요일', '화요일', '월요일', '수요일', '월요일']\n",
      "{'월요일': 0, '화요일': 1, '수요일': 2}\n",
      "[3, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "word2index, bow = bow_process('월요일 화요일 월요일 수요일 월요일')\n",
    "print(word2index)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이재명', '더불어', '민주당', '대선', '후보', '가', '윤석열', '국민', '의', '힘', '대선', '후보', '와', '오차', '범위', '인', '1', '포인트', '격차', '로', '앞서며', '초', '접전', '을', '벌이는', '것', '으로', '나타났다']\n",
      "{'이재명': 0, '더불어': 1, '민주당': 2, '대선': 3, '후보': 4, '가': 5, '윤석열': 6, '국민': 7, '의': 8, '힘': 9, '와': 10, '오차': 11, '범위': 12, '인': 13, '1': 14, '포인트': 15, '격차': 16, '로': 17, '앞서며': 18, '초': 19, '접전': 20, '을': 21, '벌이는': 22, '것': 23, '으로': 24, '나타났다': 25}\n",
      "[1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "word2index, bow = bow_process('이재명 더불어민주당 대선후보가 윤석열 국민의힘 대선후보와 오차범위인 1%포인트 격차로 앞서며 초접전을 벌이는 것으로 나타났다.')\n",
    "print(word2index)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer 클래스로 BoW 만들기\n",
    "- 공백을 구분값으로하여 빈도수를 산출한다.\n",
    "- 2개 이상의 단어를 대상으로 빈도수를 산출함.\n",
    "- 문서를 토큰 리스트로 변환한다.\n",
    "- 각 문서에서 토큰의 출현 빈도를 센다.\n",
    "- 각 문서를 BOW 인코딩 벡터로 변환한다.\n",
    "- min_df=0: 빈도수가 설정된 값보다 작으면 사용하지 않음,0은 모든 빈도 사용, 문서를 구성하는 문장 기준 예) 2개의 문장에서 2번 발생한 단어 필터링시 선언: min_df=2 \n",
    "- ngram_range=(1, 1): 빈도수를 계산할 단어를 조합할 문자의 수\n",
    "- CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행\n",
    "- 영어의 경우 띄어쓰기만으로 토큰화가 수행되기 때문에 문제가 없음.\n",
    "- 한국어에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않을 수 있음\n",
    "- CountVectorizer는 띄어쓰기를 기준으로 분리한 뒤에 '물가상승률과'와 '물가상승률은'으로 조사를 포함해서<br>\n",
    "  하나의 단어로 판단하기 때문에 서로 다른 두 단어로 인식합니다.<br>\n",
    "  그리고 '물가상승률과'와 '물가상승률은'이 각자 다른 인덱스에서 1이라는 빈도의 값을 갖게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 말뭉치: 처리하려는 모든 문장/단어들의 집합\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray()) # 단어의 빈도 수 산출\n",
    "# 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다. 알파벳 순서\n",
    "print(vector.vocabulary_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어를 제거한 BoW 만들기\n",
    "- 불용어를 제거하는 일은 자연어 처리의 정확도를 높이기 위해서 선택할 수 있는 전처리 기법임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "# (1) 사용자가 직접 정의한 불용어 사용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print(vect.fit_transform(text).toarray())  # 단어의 빈도 수 산출\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "# (2) CountVectorizer에서 제공하는 자체 불용어 사용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\") # 불용어 대상을 영어로 지정\n",
    "print(vect.fit_transform(text).toarray())    # 단어의 빈도 수 산출\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\soldesk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3) NLTK에서 지원하는 불용어 사용\n",
    "import nltk \n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "# 불용어 지원 언어 목록 확인 폴더: C:\\Users\\stu\\AppData\\Roaming\\nltk_data\\corpora\\stopwords\n",
    "sw = stopwords.words(\"english\") # 영어 관련 불용어 자동 제공됨.\n",
    "vect = CountVectorizer(stop_words =sw)\n",
    "print(vect.fit_transform(text).toarray()) # 단어의 빈도 수 산출\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 2]]\n",
      "{'사과에': 3, '딸기': 1, '그리고': 0, '딸기와': 2, '포도': 4}\n"
     ]
    }
   ],
   "source": [
    "text=[\"사과에 딸기 그리고 딸기와 포도 포도\"]\n",
    "vect = CountVectorizer()\n",
    "print(vect.fit_transform(text).toarray())  # 단어의 빈도 수 산출\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "[[1 0 1 0]\n",
      " [1 1 0 0]\n",
      " [0 0 0 2]]\n",
      "{'사과': 2, '딸기': 0, '바나나': 1, '수박': 3}\n"
     ]
    }
   ],
   "source": [
    "text = ['사과 딸기', '딸기 바나나', '수박 수박'] # 문장 3개\n",
    "# min_df=0: 모든 빈도 포함, ngram_range=(1, 1): 단어를 1개씩 분리\n",
    "vect = CountVectorizer(min_df=0, ngram_range=(1, 1))\n",
    "fruit_vector = vect.fit_transform(text) # 공백으로 값을 구분하여 빈도수 산출\n",
    "print(fruit_vector.shape) \n",
    "print(fruit_vector.toarray())\n",
    "print(vect.vocabulary_)\n",
    "#              딸기 바나나 사과 수박 <-- 단어 사전\n",
    "# 사과 딸기    1    0      1    0 \n",
    "# 딸기 바나나  1    1      0    0 \n",
    "# 수박 수박    0    0      0    2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[[1 0 1 0]\n",
      " [1 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 2]]\n",
      "{'사과': 2, '딸기': 0, '바나나': 1, '수박': 3}\n"
     ]
    }
   ],
   "source": [
    "text = ['사과 딸기', '딸기 바나나', '딸기', '수박 수박']\n",
    "# min_df=1: 빈도가 1이상 포함, ngram_range=(1, 1): 단어를 1개 분리\n",
    "vect = CountVectorizer(min_df=1, ngram_range=(1, 1))\n",
    "fruit_vector = vect.fit_transform(text) # 공백으로 값을 구분하여 빈도수 산출\n",
    "print(fruit_vector.shape) \n",
    "print(fruit_vector.toarray())\n",
    "print(vect.vocabulary_)\n",
    "#              딸기 바나나 사과 수박 \n",
    "# 사과 딸기    1    0      1    0 \n",
    "# 딸기 바나나  1    1      0    0 \n",
    "# 딸기         1    0      0    0 \n",
    "# 수박 수박    0    0      0    2 <- 포함안됨, 하나의 문장에서만 출현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[[1 0 1 0]\n",
      " [1 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 2]]\n",
      "{'사과': 2, '딸기': 0, '바나나': 1, '수박': 3}\n"
     ]
    }
   ],
   "source": [
    "# 하나의 글자로 구성된 '배'는 제외됨.\n",
    "text = ['사과 딸기', '딸기 바나나', '배', '수박 수박']\n",
    "# min_df=0: 빈도가 0이상 포함, ngram_range=(1, 1): 단어를 1개 분리\n",
    "vect = CountVectorizer(min_df=0, ngram_range=(1, 1))\n",
    "fruit_vector = vect.fit_transform(text) # 공백으로 값을 구분하여 빈도수 산출\n",
    "print(fruit_vector.shape) \n",
    "print(fruit_vector.toarray())\n",
    "print(vect.vocabulary_)\n",
    "#              딸기 바나나 사과 수박 \n",
    "# 사과 딸기    1    0      1    0 \n",
    "# 딸기 바나나  1    1      0    0 \n",
    "# 배           0    0      0    0 \n",
    "# 수박 수박    0    0      0    2 <- 포함안됨, 하나의 문장에서만 출현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "{'딸기': 0}\n"
     ]
    }
   ],
   "source": [
    "# min_df=2: 빈도가 2이상 포함, ngram_range=(1, 1): 단어를 1개 분리\n",
    "text = ['사과 딸기', '딸기 바나나', '배', '수박 수박']\n",
    "vect = CountVectorizer(min_df=2, ngram_range=(1, 1))\n",
    "fruit_vector = vect.fit_transform(text) # 공백으로 값을 구분하여 빈도수 산출\n",
    "print(fruit_vector.shape) \n",
    "print(fruit_vector.toarray()) # 공백으로 값을 구분하여 빈도수 출력\n",
    "print(vect.vocabulary_)\n",
    "#              딸기 <- 4개의 문장에 등장한 빈도수가 2번 이상인 단어(토큰)\n",
    "# 사과 딸기    1  \n",
    "# 딸기 바나나  1  \n",
    "# 배           0 \n",
    "# 수박 수박    0 <- 하나의 문장에서 빈도수는 2번이나 문장 기준 전체 문서에서는 1번 등장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 7)\n",
      "[[1 0 0 1 1 0 0]\n",
      " [1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 3 2]]\n",
      "{'사과': 3, '딸기': 0, '사과 딸기': 4, '바나나': 2, '딸기 바나나': 1, '수박': 5, '수박 수박': 6}\n"
     ]
    }
   ],
   "source": [
    "# 단어를 1~2개 분리\n",
    "text = ['사과 딸기', '딸기 바나나', '수박', '수박 수박 수박']\n",
    "# min_df=2: 빈도가 2이상 포함, ngram_range=(1, 2): 단어를 1~2개 분리\n",
    "vect = CountVectorizer(min_df=0, ngram_range=(1, 2))\n",
    "fruit_vector = vect.fit_transform(text) # 공백으로 값을 구분하여 빈도수 산출\n",
    "print(fruit_vector.shape) \n",
    "print(fruit_vector.toarray())\n",
    "print(vect.vocabulary_)\n",
    "#             '딸기' '딸기 바나나' '바나나' '사과' '사과 딸기' '수박' '수박 수박'\n",
    "# 사과 딸기    1                             1\n",
    "# 딸기 바나나  1      1             1\n",
    "# 수박                                                          1\n",
    "# 수박 수박 수박                                                3      2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
