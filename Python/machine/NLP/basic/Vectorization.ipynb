{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '날씨', '구름']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import numpy as np\n",
    "\n",
    "komoran = Komoran()\n",
    "text = \"오늘 날씨는 구름이 많아요.\"\n",
    "\n",
    "# 명사만 추출\n",
    "nouns = komoran.nouns(text)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dics.keys(): dict_keys([])\n",
      "len(dics): 0\n",
      "dics.keys(): dict_keys(['오늘'])\n",
      "len(dics): 1\n",
      "dics.keys(): dict_keys(['오늘', '날씨'])\n",
      "len(dics): 2\n",
      "{'오늘': 0, '날씨': 1, '구름': 2}\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 구축 및 단어별 인덱스 부여, 정수화\n",
    "dics = {}\n",
    "for word in nouns: # ['오늘', '날씨', '구름']\n",
    "    if word not in dics.keys():\n",
    "        print('dics.keys():', dics.keys())\n",
    "        print('len(dics):', len(dics))\n",
    "        # dics dictionary에 word란 키로 dictionary 길이 할당\n",
    "        dics[word] = len(dics) # dics['오늘'] = 0\n",
    "        \n",
    "print(dics)\n",
    "print(list(dics.values())) # 값만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 원-핫 인코딩\n",
    "nb_classes = len(dics)        # dictionary 길이\n",
    "print(type(np.eye(nb_classes)))\n",
    "print(np.eye(nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = list(dics.values()) # [0, 1, 2]\n",
    "one_hot_targets = np.eye(nb_classes)[targets]\n",
    "print(one_hot_targets)\n",
    "print()\n",
    "one_hot_targets = np.eye(nb_classes)[[2, 1, 1]]\n",
    "print(one_hot_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8112052', '어릴때보고 지금다시봐도 재밌어요ㅋㅋ', '1']\n",
      "['8132799', '디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.', '1']\n"
     ]
    }
   ],
   "source": [
    "line = \"8112052\t어릴때보고 지금다시봐도 재밌어요ㅋㅋ\t1\"\n",
    "print(line.split('\\t'))\n",
    "line = \"8132799\t디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.\t1\"\n",
    "print(line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Komoran\n",
    "import time\n",
    "\n",
    "\n",
    "# 네이버 영화 리뷰 데이터 읽어옴\n",
    "def read_review_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:] # header 제거\n",
    "    return data # 2차원 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 말뭉치 데이터 읽기 시작\n",
      "2) 리뷰 데이터 전체 개수: 200000\n",
      "3) 말뭉치 데이터 읽기 완료:  0.5968971252441406\n",
      "[['8112052', '어릴때보고 지금다시봐도 재밌어요ㅋㅋ', '1'], ['8132799', '디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.', '1'], ['4655635', '폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.', '1'], ['9251303', '와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런게 진짜 영화지', '1'], ['10067386', '안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.', '1']]\n"
     ]
    }
   ],
   "source": [
    "# 측정 시작\n",
    "start = time.time()\n",
    "\n",
    "# 리뷰 파일 읽어오기, 말뭉치: 문장의 모임, 분석 대상의 전체\n",
    "print('1) 말뭉치 데이터 읽기 시작')\n",
    "review_data = read_review_data('./data/ratings.txt')\n",
    "print('2) 리뷰 데이터 전체 개수:', len(review_data)) # 리뷰 데이터 전체 개수\n",
    "print('3) 말뭉치 데이터 읽기 완료: ', time.time() - start)\n",
    "print(review_data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8112052', '8132799']\n",
      "\n",
      "['어릴때보고 지금다시봐도 재밌어요ㅋㅋ', '디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.']\n"
     ]
    }
   ],
   "source": [
    "print([sentence[0] for sentence in review_data[0:2]]) # 첫번째 변수\n",
    "print()\n",
    "print([sentence[1] for sentence in review_data[0:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) 형태소에서 명사만 추출 시작\n",
      "2) 형태소에서 명사만 추출 완료:  177.18568515777588\n",
      "[['때'], ['디자인', '학생', '외국', '디자이너', '전통', '발전', '문화', '산업', '사실', '우리나라', '시절', '끝', '열정', '노라', '노', '전통', '사람', '꿈', '수', '것', '감사']]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 문장단위로 명사만 추출해 학습 입력 데이터로 만듬\n",
    "print('2) 형태소에서 명사만 추출 시작')\n",
    "komoran = Komoran() # 형태소 분석기의 객체 생성\n",
    "# sentence[1]: document, 리뷰, 시간이 많이 필요함 core 6: 85초, i5 2320: 177\n",
    "docs = [komoran.nouns(sentence[1]) for sentence in review_data]\n",
    "print('2) 형태소에서 명사만 추출 완료: ', time.time() - start)\n",
    "print(docs[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./ratings_docs.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pickle # 파이썬 자료형 저장 지원\n",
    "import joblib # 병렬 처리 지원, 모델 저장, 로딩\n",
    "\n",
    "joblib.dump(docs, './ratings_docs.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['때'], ['디자인', '학생', '외국', '디자이너', '전통', '발전', '문화', '산업', '사실', '우리나라', '시절', '끝', '열정', '노라', '노', '전통', '사람', '꿈', '수', '것', '감사']]\n"
     ]
    }
   ],
   "source": [
    "docs2 = joblib.load('./ratings_docs.pkl') \n",
    "print(docs2[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) word2vec 모델 학습 시작\n",
      "3) word2vec 모델 학습 완료:  15.55300784111023\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# word2vec 모델 학습\n",
    "print('3) word2vec 모델 학습 시작')\n",
    "# sentences=docs: 명사 토큰의 집합\n",
    "# vector_size=200: 200개의 변수로 docs에 존재하는 토큰을 수치(벡터)화 함 \n",
    "# window=4: 주변단어 4개까지 확인\n",
    "# min_count=2: 빈도수 2이하의 단어들은 학습하지 않음.\n",
    "# sg: 0(CBOW 모델), 1(skip-gram 모델)\n",
    "model = Word2Vec(sentences=docs, vector_size=200, window=4, min_count=2, sg=1)\n",
    "print('3) word2vec 모델 학습 완료: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4) 학습된 모델 저장 시작\n",
      "4) 학습된 모델 저장 완료:  0.07805061340332031\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 모델 저장\n",
    "print('4) 학습된 모델 저장 시작')\n",
    "model.save('./ratings.model')\n",
    "print('4) 학습된 모델 저장 완료: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_count :  200000\n",
      "corpus_total_words :  1076896\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# 학습된 말뭉치 개수, 말뭉치(코퍼스) 내 전체 단어 개수\n",
    "print(\"corpus_count : \", model.corpus_count) # 문장수\n",
    "print(\"corpus_total_words : \", model.corpus_total_words) # 단어수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_total_words :  1076896\n",
      "일요일 :  200 [ 0.09601194 -0.06301533  0.05377556  0.05146767  0.08600172 -0.18279502\n",
      "  0.11715929  0.23092006  0.02606846  0.07284421  0.02399201 -0.2582403\n",
      "  0.1249833   0.05983743  0.09475879  0.10584436 -0.14510472 -0.1474172\n",
      "  0.11248036 -0.08936764  0.2642022  -0.09662985 -0.13528572  0.09430071\n",
      "  0.07115527 -0.05240937  0.06333485  0.04684192 -0.10860016 -0.17254148\n",
      "  0.18870533  0.01000663  0.02182337 -0.10107178  0.02372091 -0.01131875\n",
      "  0.26405787  0.06825031 -0.07916279 -0.02596795 -0.10694709 -0.06646781\n",
      " -0.01669144  0.0707752   0.07875628  0.05340417 -0.17502826  0.00176028\n",
      "  0.19976006  0.16485247  0.05665494 -0.02617382 -0.10000215  0.10478491\n",
      "  0.02070545  0.15489574  0.1910311  -0.09852612 -0.00601866  0.11823366\n",
      "  0.05474098  0.0406172   0.1503287   0.1298537  -0.1972055   0.0092661\n",
      " -0.10677249  0.10701106 -0.11345098  0.11067979 -0.01895295 -0.00505385\n",
      "  0.26622763 -0.01686645 -0.07012222 -0.01199859  0.07126155 -0.16521606\n",
      " -0.24455792 -0.09000716 -0.11763846  0.11380373 -0.09491372  0.02737256\n",
      " -0.1883666  -0.08917926  0.03052771  0.10466183  0.15325165  0.05986702\n",
      "  0.15127422  0.14750712  0.01145986  0.19528586  0.25530535  0.06881288\n",
      " -0.07335338 -0.18917675 -0.0617114  -0.04594249 -0.21321408  0.06380018\n",
      "  0.25798568 -0.00775156 -0.04485777 -0.28133118  0.14448063  0.08435404\n",
      " -0.00991399 -0.26947346 -0.14196551 -0.2444213  -0.04867702 -0.00881785\n",
      "  0.21610361  0.0075781   0.08311815 -0.25472662  0.04223718 -0.21711987\n",
      "  0.31158033  0.34040597 -0.0715057  -0.30103526 -0.01519575  0.04345785\n",
      " -0.08523839  0.07224468  0.07826549 -0.001189    0.02755156  0.15884134\n",
      " -0.10154629  0.02846446 -0.04785954  0.20472208  0.13158867 -0.1159748\n",
      " -0.19405468 -0.08302121  0.23469695 -0.12345706  0.06526864 -0.11403916\n",
      " -0.04108029 -0.04918201 -0.23050095  0.1695036  -0.01013855 -0.00695061\n",
      "  0.04114684 -0.21904486  0.11560761 -0.03188451 -0.16230401  0.2908544\n",
      "  0.05259477  0.20967881 -0.03038084  0.00057239  0.09848896  0.08342258\n",
      " -0.143528   -0.03388856 -0.04977681  0.04440951 -0.0636185  -0.05416694\n",
      "  0.12500378  0.10236124 -0.16850531  0.10629275  0.02492808 -0.13954937\n",
      "  0.05649578  0.13504839  0.11228527  0.04599335 -0.05813868 -0.03009837\n",
      "  0.09370433  0.0099432  -0.0292636  -0.02314617 -0.01316278  0.18805869\n",
      " -0.20758425 -0.0056398   0.24222393  0.06797364  0.26841313 -0.12564811\n",
      " -0.10136449 -0.14371791  0.04427076 -0.03442024 -0.01242461 -0.14720006\n",
      "  0.05987068  0.07629114]\n"
     ]
    }
   ],
   "source": [
    "# 모델 로딩\n",
    "model = Word2Vec.load('./ratings.model')\n",
    "print(\"corpus_total_words : \", model.corpus_total_words)\n",
    "\n",
    "# '사랑'이란 단어로 생성한 단어 임베딩 벡터, 200개의 실수값으로 '사랑' 표현\n",
    "print('일요일 : ', len(model.wv['일요일']), model.wv['일요일'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일요일 = 월요일\t 0.8930642\n",
      "안성기 = 배우\t 0.719242\n",
      "눈물 = 웃음\t 0.6519672\n",
      "배우 = 연기\t 0.61714625\n",
      "명동 = 쇼핑\t 0.92532635\n"
     ]
    }
   ],
   "source": [
    "# 단어 유사도 계산, 1에 가까울수록 유사도가 높음\n",
    "# 좋은 말뭉치 모델을 사용시 언어 학습이 완성도있게 이루어짐.\n",
    "print(\"일요일 = 월요일\\t\", model.wv.similarity(w1='일요일', w2='월요일'))\n",
    "print(\"안성기 = 배우\\t\", model.wv.similarity(w1='안성기', w2='배우'))\n",
    "print(\"눈물 = 웃음\\t\", model.wv.similarity(w1='눈물', w2='웃음'))\n",
    "print(\"배우 = 연기\\t\", model.wv.similarity(w1='배우', w2='연기'))\n",
    "print(\"명동 = 쇼핑\\t\", model.wv.similarity(w1='명동', w2='쇼핑'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('한석규', 0.9403318762779236), ('김수로', 0.9400565028190613), ('정재영', 0.9369944334030151), ('최민식', 0.9352214932441711), ('지진희', 0.9331547617912292)]\n",
      "[('엑스맨', 0.8179543018341064), ('스타워즈', 0.8018988370895386), ('포터', 0.7916746735572815), ('다이하드', 0.790935218334198), ('반지의 제왕', 0.7854950428009033)]\n"
     ]
    }
   ],
   "source": [
    "# 가장 유사한 단어 추출\n",
    "print(model.wv.most_similar(\"안성기\", topn=5))\n",
    "print(model.wv.most_similar(\"시리즈\", topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
