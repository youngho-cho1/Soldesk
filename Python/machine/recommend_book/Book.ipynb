%autosave 0
import warnings
warnings.filterwarnings(action='ignore')

import os

import numpy as np
import pandas as pd
import seaborn as sb

from IPython.display import Image

from tensorflow.keras.models import Sequential  # class
from tensorflow.keras.models import load_model  # model 사용
from tensorflow.keras.layers import Dense       # 전결합
from tensorflow.keras.layers import Dropout     # 특정 node를 사용안함.
from tensorflow.keras.callbacks import EarlyStopping   # 학습 자동 중지
from tensorflow.keras.callbacks import ModelCheckpoint # 우수한 학습 모델 파일 저장
from tensorflow.keras import regularizers 
from tensorflow.keras.utils import to_categorical   # one-hot 엔코딩
from tensorflow.keras.optimizers import Adam    # 가중치, bias 최적화

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split # 학습셋과 테스트셋의 분리 지원
from sklearn.model_selection import StratifiedKFold  # K겹 교차 검증

import matplotlib.pyplot as plt

from matplotlib import font_manager, rc

import platform 

if (platform.system() == 'Windows'):  # Windows, Linux, Darwin
    rc('font', family=font_manager.FontProperties(fname="C:/Windows/Fonts/malgun.ttf").get_name())
    path = '.' # Local
else:    
    plt.rc('font', family='NanumBarunGothic')  # Ubuntu 18.04 기준 한글 처리
    path = '/content/drive/My Drive/ai7/dnn/recommendation' # Colab

plt.rcParams["font.size"] = 12         # 글자 크기
# plt.rcParams["figure.figsize"] = (10, 4) # 10:4의 그래프 비율
plt.rcParams['axes.unicode_minus'] = False  # minus 부호는 unicode 적용시 한글이 깨짐으로 설정

%matplotlib inline
# header가 있을경우 skiprows=1 선언
# data = np.loadtxt(path + '/train1.csv', delimiter=',', skiprows=1, dtype=np.float64)  # 특성이 작은 데이터
# data = np.loadtxt(path + '/train2.csv', delimiter=',', skiprows=1, dtype=np.float64)   # 특성이 작은 데이터의 예외 추가
# data = np.loadtxt(path + '/train3.csv', delimiter=',', skiprows=1, dtype=np.float64)   # 특성이 작은 데이터의 예외 추가
data = np.loadtxt(path + '/train4.csv', delimiter=',', skiprows=1, dtype=np.float64)   # 특성이 작은 데이터의 예외 추가
print(type(data))
print(data.shape)

# 데이터와 class의 분리
# 1,1,1,0,0,0,0,0,0,0
# 0: 개발 관련 도서
# 1: 해외 여행 관련 도서
# 2: 소설 관련 도서
X = data[:, 0:9]  # 0 ~ 8
print(X.shape)
Y = data[:, 9]    # 10 번째 데이터, class의 분리
print(Y.shape)

print(Y)

Y = Y.astype('int') # 정수로 형변환
print(Y)

# 0: 개발 관련 도서
# 1: 해외 여행 관련 도서
# 2: 소설 관련 도서
Y_encoded = to_categorical(Y) # one-hot-encoding

print(Y_encoded) 

print(X[0])
print(Y_encoded[0])

# train_test_split 분할을 통한 훈련, 검증, 테스트 데이터의 분리
seed = 0
# 90%: 분할대기, 10%: 테스트
x_train_all, x_test, y_train_all, y_test = train_test_split(X, Y_encoded,
                                           stratify=Y_encoded,
                                           test_size=0.1,
                                           random_state=seed)
# 나머지 데이터 90%를 분할, 80%: 훈련, 20%: 검증
x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all,
                                           stratify=y_train_all,
                                           test_size=0.2,
                                           random_state=seed)

print(y_val)
print(y_val.shape)
# Iris-setosa: 0, Iris-versicolor: 1, Iris-virginica: 2
# (7, 3): 7개의 데이터가 입력되어 한건당 3가지에 속할 확률이 출력됨으로 7행 3열이됨.

print(y_test)
print(y_test.shape)
# (4, 3): 4개의 데이터가 입력되어 한건당 3가지에 속할 확률이 출력됨으로 4행 3열이됨.

model = Sequential()

# 네트워크 구성
# model.add(Dense(20, input_shape=(9, ), activation='relu'))
model.add(Dense(5, input_dim=9, activation='relu'))
model.add(Dense(3, activation='softmax')) # 입력: 1,0,0,1,0,0,1,0,0 → 0 ~ 1 사이의 확률 3가지 출력, 총합은 1
model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy', 
                                metrics=['accuracy'])
model.summary()

mcp = ModelCheckpoint(filepath='./Book.h5', monitor='val_accuracy', verbose=1, save_best_only=True)

es = EarlyStopping(monitor='loss', patience=1, restore_best_weights=True)

hist = model.fit(x_train, y_train, validation_data=(x_val, y_val), shuffle=True,
                 epochs=50, batch_size=1, callbacks=[mcp, es])

import matplotlib.pyplot as plt
%matplotlib inline  

fig, loss_ax = plt.subplots()
# plt.figure(figsize=(6,4)) # ERROR
fig.set_size_inches(10, 5)  # 챠트 크기 설정

acc_ax = loss_ax.twinx()  # 오른쪽 y 출 설정

# 왼쪽 y 축 설정
loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')
loss_ax.set_ylim([0.0, 1.5]) # 값을 반영하여 변경

# 오른쪽 y 축 설정
acc_ax.plot(hist.history['accuracy'], 'b', label='train accuracy')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='val accuracy')
acc_ax.set_ylim([0.0, 1.0])

# 축 레이블 설정
loss_ax.set_xlabel('epoch' )  # 학습 횟수
loss_ax.set_ylabel('loss')    # 왼쪽 y 축 레이블, 오차
acc_ax.set_ylabel('accuracy') # 오른쪽 y 축 레이블,정확도

loss_ax.legend(loc='upper left') # 왼쪽 y 축 오차 레이블 위치
acc_ax.legend(loc='lower left')  # 오른쪽 y 축 정확도 레이블 위치

plt.show()

test_loss, test_acc = model.evaluate(x_val, y_val, batch_size=1, verbose=0)
print('손실값:', test_loss, '/정확도:', (test_acc*100), '%')

print('데이터:', x_test.shape) # 변수가 9개로 구성된 4건의 관측치(행)
print('데이터:', x_test[0])    # 첫번째 데이터행

p = model.predict(x_test)      # 테스트 데이터 4건 ★
print('예측 결과 p.shape:', p.shape)     # (4, 3): 3: 폼종의 갯수

print('예측값:', p[0])        # 첫번째 예측값 출력, 확률 0 ~ 1사이의 실수값
print('예측값의 합: {0:0.3f}'.format(np.sum(p[0])))
print('예측값: {0:.5f}% {1:.5f}% {2:.5f}%'.format(p[0,0]*100,p[0,1]*100,p[0,2]*100))
print('One-hot-encoding: ', y_test[0])
print(np.argmax(p[0]))      # 가장 큰값의 index

print(p)
